Qwery
===============
Simply put, Qwery generates Spark-Scala SBT projects (build.sbt and all!) from SQL queries; prioritizing
Hive, Athena and Netezza SQL compatibility. Qwery literally compiles your SQL and generates 
Spark/Scala code without the typical massaging that is required to get things running in Spark!

### Table of Contents

* <a href="#motivation">Motivation</a>
* <a href="#features">Features</a>
* <a href="#development">Development</a>
    * <a href="#build-requirements">Build Requirements</a>
    * <a href="#build-application">Building the application</a>
    * <a href="#run-tests">Running the tests</a>
* <a href="#examples">Examples</a>

<a name="motivation"></a>
### Motivation

Many organizations have standardized their environments with Spark for 
Big Data and Machine learning processes, yet have a large backlog of legacy applications
(Hive, Netezza and others) that require a painful migration to native Spark and Spark SQL code.
Having done much of this work myself, I've decided to build an application that almost fully 
automates that conversion process. 

Just run Qwery to generate your project, do ```sbt assembly``` then deploy your fat jar to AWS EMR or other Spark container.

<a name="features"></a>
### Features

Qwery has two methods of code generation:
* Native Spark/Scala code (e.g. ```df.filter($"id".isNotNull) ```)
* Spark SQL

<a name="development"></a>
### Development

<a name="build-requirements"></a>
#### Build Requirements

* [SBT v0.13.18](http://www.scala-sbt.org/download.html)

<a name="build-application"></a>
#### Building the application

```bash
sbt clean assembly
```

<a name="run-tests"></a>
#### Running the tests

```bash
sbt clean test
```

<a name="examples"></a>
### Examples

Consider the following SQL snippet:

```sql
begin

    /* First, we define our input and output sources */

    info 'Loading the input and output sources... ';
    include './samples/sql/companylist/companylist-input.sql';
    include './samples/sql/companylist/companylist-output-json.sql';

    /* Show some data */

    show (
        select Symbol, Name, Sector, Industry, SummaryQuote
        from Securities
        where Industry = 'Oil/Gas Transmission'
    ) limit 5;

end
```

Here's the code generated by Qwery:

```scala
package com.coxautoinc.maid.adbook

import com.qwery.models.{StorageFormats, _}
import com.qwery.platform.sparksql.generator.ResourceManager
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import org.slf4j.LoggerFactory

class CompanyListSparkJob() extends Serializable {
  @transient
  private lazy val logger = LoggerFactory.getLogger(getClass)

  def start(args: Array[String])(implicit spark: SparkSession): Unit = {
    logger.info("Loading the input and output sources... ")
    ResourceManager.add(Table(
      name = "Securities",
      columns = List(
        Column(name = "Symbol", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "Name", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "LastSale", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "MarketCap", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "IPOyear", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "Sector", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "Industry", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "SummaryQuote", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "Reserved", `type` = ColumnTypes.STRING, isNullable = true)),
      location = "./samples/companylist/csv/",
      fieldDelimiter = Some(","),
      fieldTerminator = None,
      headersIncluded = Some(true),
      nullValue = Some("n/a"),
      inputFormat = Some(StorageFormats.CSV),
      outputFormat = None,
      partitionColumns = List(),
      properties = Map(),
      serdeProperties = Map()
    ))
    ResourceManager.add(Table(
      name = "OilGasSecurities",
      columns = List(
        Column(name = "Symbol", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "Name", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "LastSale", `type` = ColumnTypes.DOUBLE, isNullable = true)
        , Column(name = "MarketCap", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "IPOyear", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "Sector", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "Industry", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "SummaryQuote", `type` = ColumnTypes.STRING, isNullable = true)
        , Column(name = "Reserved", `type` = ColumnTypes.STRING, isNullable = true)),
      location = "./temp/flink/companylist/json/",
      fieldDelimiter = None,
      fieldTerminator = None,
      headersIncluded = None,
      nullValue = None,
      inputFormat = None,
      outputFormat = Some(StorageFormats.JSON),
      partitionColumns = List(),
      properties = Map(),
      serdeProperties = Map()
    ))
    spark.sql(
      """|
         |SELECT
         |Symbol,
         |Name,
         |Sector,
         |Industry,
         |SummaryQuote
         |FROM global_temp.Securities
         |WHERE Industry = 'Oil/Gas Transmission'
         |""".stripMargin('|')).show(5)
  }

}

object CompanyListSparkJob {
  private[this] val logger = LoggerFactory.getLogger(getClass)

  def main(args: Array[String]): Unit = {
    implicit val spark: SparkSession = createSparkSession("AdBook_PoC")
    new CompanyListSparkJob().start(args)
    spark.stop()
  }

  def createSparkSession(appName: String): SparkSession = {
    val sparkConf = new SparkConf()
    val builder = SparkSession.builder()
      .appName(appName)
      .config(sparkConf)
      .enableHiveSupport()

    // first attempt to create a clustered session
    try builder.getOrCreate() catch {
      // on failure, create a local one...
      case _: Throwable =>
        System.setSecurityManager(null)
        logger.warn(s"$appName failed to connect to EMR cluster; starting local session...")
        builder.master("local[*]").getOrCreate()
    }
  }
}
```